{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read CSV file with 'ISO-8859-1' encoding\n",
    "df = pd.read_csv('../Dataset/train.csv', encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and convert to UTF-8\n",
    "def clean_text(text):\n",
    "    # Remove unsupported characters\n",
    "    cleaned_text = ''.join(char for char in text if ord(char) < 128)\n",
    "    # Encode to UTF-8\n",
    "    utf8_text = cleaned_text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "    return utf8_text\n",
    "\n",
    "# Apply cleaning function to specific columns in DataFrame\n",
    "df['summary'] = df['summary'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned DataFrame to a new CSV file in UTF-8 format\n",
    "df.to_csv('cleaned_file.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "# Load data from CSV file\n",
    "df = pd.read_csv('train.csv')  # Replace 'your_file.csv' with the path to your CSV file\n",
    "\n",
    "# Extract articles and summaries from the DataFrame\n",
    "articles = df['article'].tolist()\n",
    "summaries = df['summary'].tolist()\n",
    "\n",
    "# Tokenize articles and summaries\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "tokenized_articles = tokenizer(articles, truncation=True, padding=True)\n",
    "tokenized_summaries = tokenizer(summaries, truncation=True, padding=True)\n",
    "\n",
    "# Create tokenized dataset\n",
    "tokenized_dataset = {\n",
    "    'input_ids': tokenized_articles.input_ids,\n",
    "    'attention_mask': tokenized_articles.attention_mask,\n",
    "    'labels': tokenized_summaries.input_ids,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your CSV file using pandas\n",
    "df = pd.read_csv('train.csv', encoding='ISO-8859-1')\n",
    "\n",
    "# Create a custom dataset using the 'article' column\n",
    "custom_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Tokenize the custom dataset\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    inputs = tokenizer(example['article'], padding='max_length', truncation=True, return_tensors='pt')\n",
    "    inputs['labels'] = inputs.input_ids.clone()\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = custom_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='Results',          # Output directory for the model\n",
    "    num_train_epochs=3,            # Number of training epochs\n",
    "    per_device_train_batch_size=1, # Batch size per device during training\n",
    "    save_steps=500,                # Save model every 500 steps\n",
    "    save_total_limit=2,            # Only last 2 models are saved. Older ones are deleted.\n",
    ")\n",
    "\n",
    "# Load pre-trained T5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_path = 'Results'  # Path to the directory where your trained model is saved\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/content/Results/checkpoint-1500\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"Ever noticed how plane seats appear to be getting smaller and smaller? With increasing numbers of people taking to the skies, some experts are questioning if having such packed out planes is putting passengers at risk. They say that the shrinking space on aeroplanes is not only uncomfortable - it's putting our health and safety in danger. More than squabbling over the arm rest, shrinking space on planes putting our health and safety in danger? This week, a U.S consumer advisory group set up by the Department of Transportation said at a public hearing that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. 'In a world where animals have more rights to space and food than humans,' said Charlie Leocha, consumer representative on the committee.Â 'It is time that the DOT and FAA take a stand for humane treatment of passengers.' But could crowding on planes lead to more serious issues than fighting for space in the overhead lockers, crashing elbows and seat back kicking? Tests conducted by the FAA use planes with a 31 inch pitch, a standard which on some airlines has decreased . Many economy seats on United Airlines have 30 inches of room, while some airlines offer as little as 28 inches . Cynthia Corbertt, a human factors researcher with the Federal Aviation Administration, that it conducts tests on how quickly passengers can leave a plane. But these tests are conducted using planes with 31 inches between each row of seats, a standard which on some airlines has decreased, reported the Detroit News. The distance between two seats from one point on a seat to the same point on the seat behind it is known as the pitch. While most airlines stick to a pitch of 31 inches or above, some fall below this. While United Airlines has 30 inches of space, Gulf Air economy seats have between 29 and 32 inches, Air Asia offers 29 inches and Spirit Airlines offers just 28 inches. British Airways has a seat pitch of 31 inches, while easyJet has 29 inches, Thomson's short haul seat pitch is 28 inches, and Virgin Atlantic's is 30-31.\"\n",
    "\n",
    "# Tokenize the article\n",
    "inputs = tokenizer.encode(\"summarize: \" + article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load your trained model and tokenizer\n",
    "model_path = 'Results'  # Path to the directory where your trained model is saved\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/content/Results/checkpoint-1500\")\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large', model_max_length=1024)\n",
    "\n",
    "# Load articles from the CSV file\n",
    "df = pd.read_csv('/content/train.csv', encoding='utf-8')  # Replace 'your_input_file.csv' with your CSV file path\n",
    "articles = df['article'].tolist()\n",
    "\n",
    "# Generate summaries for each article\n",
    "summaries = []\n",
    "for article in articles:\n",
    "    inputs = tokenizer.encode(\"summarize: \" + article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Add the summaries to the DataFrame\n",
    "df['summary'] = summaries\n",
    "\n",
    "# Save the DataFrame with summaries to a new CSV file\n",
    "df.to_csv('output_file.csv', index=False, encoding='utf-8')  # 'output_file.csv' is the name of the new CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harsh/anaconda3/envs/SMA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cumulative BERT Scores:\n",
      "Precision: 0.7828612419366836\n",
      "Recall: 0.8422643060684204\n",
      "F1 Score: 0.8110984798669815\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the BERT score model\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv(\"../Dataset/final_q3.csv\")\n",
    "\n",
    "# Extract actual and predicted values from the DataFrame\n",
    "predictions = df[\"Predicted\"].tolist()\n",
    "references = df[\"Actual\"].tolist()\n",
    "\n",
    "# Compute BERT scores for each pair of actual and predicted values\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "# Extract individual BERT scores (precision, recall, F1) for each pair\n",
    "precision = results[\"precision\"]\n",
    "recall = results[\"recall\"]\n",
    "f1 = results[\"f1\"]\n",
    "\n",
    "# Create a new DataFrame to store the BERT scores\n",
    "output_df = pd.DataFrame({\"Actual\": references, \"Predicted\": predictions, \"Precision\": precision, \"Recall\": recall, \"F1\": f1})\n",
    "\n",
    "# Save the BERT scores to a new CSV file\n",
    "output_df.to_csv(\"bert_scores_output.csv\", index=False)\n",
    "\n",
    "# Calculate the final cumulative BERT scores\n",
    "final_precision = sum(precision) / len(precision)\n",
    "final_recall = sum(recall) / len(recall)\n",
    "final_f1 = sum(f1) / len(f1)\n",
    "\n",
    "# Print the final cumulative BERT scores\n",
    "print(\"Final Cumulative BERT Scores:\")\n",
    "print(\"Precision:\", final_precision)\n",
    "print(\"Recall:\", final_recall)\n",
    "print(\"F1 Score:\", final_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: 0.36774779748743025\n",
      "ROUGE-2: 0.1667095171355023\n",
      "ROUGE-L: 0.235075239718949\n",
      "ROUGE-Lsum: 0.23484841763519826\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the ROUGE score model\n",
    "rouge = load('rouge')\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv(\"../Dataset/final_q3.csv\")\n",
    "\n",
    "# Extract actual and predicted values from the DataFrame\n",
    "predictions = df[\"Predicted\"].tolist()\n",
    "references = df[\"Actual\"].tolist()\n",
    "\n",
    "# Compute ROUGE scores for each pair of actual and predicted values\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Save the ROUGE scores to a new CSV file\n",
    "output_df = pd.DataFrame({\"Actual\": references, \"Predicted\": predictions, \"ROUGE-1\": results['rouge1'], \"ROUGE-2\": results['rouge2'], \"ROUGE-L\": results['rougeL'], \"ROUGE-Lsum\": results['rougeLsum']})\n",
    "\n",
    "output_df.to_csv(\"rouge_scores_output.csv\", index=False)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE Scores:\")\n",
    "print(\"ROUGE-1:\", results['rouge1'])\n",
    "print(\"ROUGE-2:\", results['rouge2'])\n",
    "print(\"ROUGE-L:\", results['rougeL'])\n",
    "print(\"ROUGE-Lsum:\", results['rougeLsum'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.2909516874985539\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Load BLEU score function from NLTK\n",
    "def compute_bleu_score(prediction, references):\n",
    "    return sentence_bleu(references, prediction)\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv(\"../Dataset/final_q3.csv\")\n",
    "\n",
    "# Extract actual and predicted values from the DataFrame\n",
    "predictions = df[\"Predicted\"].tolist()\n",
    "references_list = df[\"Actual\"].apply(lambda x: [reference.strip() for reference in x.split(',')]).tolist()\n",
    "\n",
    "# Compute BLEU scores for each pair of actual and predicted values\n",
    "bleu_scores = [compute_bleu_score(prediction, references) for prediction, references in zip(predictions, references_list)]\n",
    "\n",
    "# Create a new DataFrame to store the BLEU scores\n",
    "output_df = pd.DataFrame({\"Actual\": df[\"Actual\"], \"Predicted\": predictions, \"BLEU Score\": bleu_scores})\n",
    "\n",
    "# Save the BLEU scores to a new CSV file\n",
    "output_df.to_csv(\"bleu_scores_output.csv\", index=False)\n",
    "\n",
    "# Calculate the average BLEU score\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print the average BLEU score\n",
    "print(\"Average BLEU Score:\", average_bleu_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
